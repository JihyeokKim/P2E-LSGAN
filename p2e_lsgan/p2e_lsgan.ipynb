{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ed6817f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(b1=0.5, b2=0.999, batch_size=192, checkpoint_interval=30, dataset_prefix='data/mimic/', epoch=6000, experiment_name='p2e_wgan_gp_mimic', from_ppg=True, is_eval=True, lambda_gp=10, lr=0.0002, n_cpu=4, n_epochs=10000, ncritic=3, peaks_only=False, shuffle_testing=False, shuffle_training=True, signal_length=375)\n",
      "data/mimic/ppg_train.npy (4025, 375)\n",
      "data/mimic/ecg_train.npy (4025, 375)\n",
      "data/mimic/ecg_opeaks_train.npy (4025, 375)\n",
      "data/mimic/ecg_rpeaks_train.npy (4025, 375)\n",
      "data/mimic/ppg_eval.npy (995, 375)\n",
      "data/mimic/ecg_eval.npy (995, 375)\n",
      "data/mimic/ecg_opeaks_eval.npy (995, 375)\n",
      "data/mimic/ecg_rpeaks_eval.npy (995, 375)\n",
      "\n",
      "epoch:  6000\n",
      "rmse_mean: 0.16826072 , rmse_std: 0.06444295\n",
      "p_mean: 0.7855944504575757 , p_std: 0.1687237050036483\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanao\\anaconda3\\envs\\P2E\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3445: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from models import weights_init_normal, GeneratorUNet, Discriminator\n",
    "from data import get_data_loader\n",
    "from utils import compute_gradient_penalty, smoother, sample_images, evaluate_generated_signal_quality\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--experiment_name\", type=str,\n",
    "                    default=\"p2e_lsgan_mimic\", help=\"name of the experiment\")\n",
    "parser.add_argument(\"--dataset_prefix\", type=str,\n",
    "                    default=\"data/mimic/\", help=\"path to the train and valid dataset\")\n",
    "parser.add_argument(\"--epoch\", type=int, default=6000,\n",
    "                    help=\"epoch to start training from\")\n",
    "parser.add_argument(\"--shuffle_training\", type=bool,\n",
    "                    default=True, help=\"shuffle training\")\n",
    "parser.add_argument(\"--shuffle_testing\", type=bool,\n",
    "                    default=False, help=\"shuffle testing\")\n",
    "parser.add_argument(\"--is_eval\", type=bool,\n",
    "                    default=True, help=\"evaluation mode\")\n",
    "parser.add_argument(\"--from_ppg\", type=bool, default=True,\n",
    "                    help=\"reconstruct from ppg\")\n",
    "parser.add_argument(\"--peaks_only\", type=bool, default=False,\n",
    "                    help=\"L2 loss on peaks only\")\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=10000,\n",
    "                    help=\"number of epochs of training\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=192,\n",
    "                    help=\"size of the batches\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0002,\n",
    "                    help=\"adam: learning rate\")\n",
    "parser.add_argument(\"--b1\", type=float, default=0.5,\n",
    "                    help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--b2\", type=float, default=0.999,\n",
    "                    help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--lambda_gp\", type=float, default=10,\n",
    "                    help=\"Loss weight for gradient penalty\")\n",
    "parser.add_argument(\"--ncritic\", type=int, default=3,\n",
    "                    help=\" number of iterations of the critic per generator iteration\")\n",
    "parser.add_argument(\"--n_cpu\", type=int, default=4,\n",
    "                    help=\"number of cpu threads to use during batch generation\")\n",
    "parser.add_argument(\"--signal_length\", type=int,\n",
    "                    default=375, help=\"size of the signal\")\n",
    "parser.add_argument(\"--checkpoint_interval\", type=int,\n",
    "                    default=30, help=\"interval between model checkpoints\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "print(args)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "# Weighting for L2 loss\n",
    "if args.peaks_only:\n",
    "    lambda_sample = 20\n",
    "    rpeak_weight = 4\n",
    "else:\n",
    "    lambda_sample = 50\n",
    "\n",
    "# Loss functions\n",
    "if args.peaks_only:\n",
    "    criterion_samplewise = torch.nn.MSELoss(reduction='sum')\n",
    "else:\n",
    "    criterion_samplewise = torch.nn.MSELoss()\n",
    "\n",
    "# Output size of the discriminator (PatchGAN)\n",
    "patch = (1, 9)\n",
    "\n",
    "# Load data\n",
    "dataloader, val_dataloader = get_data_loader(args.dataset_prefix, args.batch_size, from_ppg=args.from_ppg,\n",
    "                                             shuffle_training=args.shuffle_training, \n",
    "                                             shuffle_testing=args.shuffle_testing)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = GeneratorUNet()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    if torch.cuda.device_count() > 2:\n",
    "        # if False:\n",
    "        generator = torch.nn.DataParallel(\n",
    "            generator, device_ids=[0, 1, 2]).to(device)\n",
    "        discriminator = torch.nn.DataParallel(\n",
    "            discriminator, device_ids=[0, 1, 2]).to(device)\n",
    "    else:\n",
    "        generator = generator.to(device)\n",
    "        discriminator = discriminator.to(device)\n",
    "\n",
    "    criterion_samplewise.to(device)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(\n",
    "    generator.parameters(), lr=args.lr, betas=(args.b1, args.b2))\n",
    "optimizer_D = torch.optim.Adam(\n",
    "    discriminator.parameters(), lr=args.lr, betas=(args.b1, args.b2))\n",
    "\n",
    "if args.epoch != 0:\n",
    "    # Load pretrained models\n",
    "\n",
    "    pretrained_path = \"saved_models/%s/multi_models_%d.pth\" % (\n",
    "        args.experiment_name, args.epoch)\n",
    "    checkpoint = torch.load(pretrained_path)\n",
    "    generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "    discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "\n",
    "    optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
    "    optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
    "\n",
    "    if args.is_eval:\n",
    "        sample_images(args.experiment_name, val_dataloader,\n",
    "                      generator, args.epoch, device)\n",
    "        evaluate_generated_signal_quality(\n",
    "            val_dataloader, generator, None, args.epoch, device)\n",
    "        sys.exit()\n",
    "else:\n",
    "    # Initialize weights\n",
    "    generator.apply(weights_init_normal)\n",
    "    discriminator.apply(weights_init_normal)\n",
    "\n",
    "os.makedirs(\"saved_models/%s\" % args.experiment_name, exist_ok=True)\n",
    "os.makedirs(\"sample_signals/%s\" % args.experiment_name, exist_ok=True)\n",
    "os.makedirs(\"logs/%s\" % args.experiment_name, exist_ok=True)\n",
    "writer = SummaryWriter(\"logs/%s\" % args.experiment_name)\n",
    "\n",
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "\n",
    "prev_time = time.time()\n",
    "for epoch in range(args.epoch+1, args.n_epochs):\n",
    "\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # Model inputs\n",
    "        real_A = batch[0].to(device)\n",
    "        real_B = batch[1].to(device)\n",
    "\n",
    "        if i % args.ncritic == 0:\n",
    "\n",
    "            # ------------------\n",
    "            #  Train Generators\n",
    "            # ------------------\n",
    "            generator.train()\n",
    "            for p in generator.parameters():\n",
    "                p.grad = None\n",
    "\n",
    "            # GAN loss\n",
    "            fake_B = generator(real_A)\n",
    "\n",
    "            # Sample-wise loss\n",
    "            if args.from_ppg and args.peaks_only:\n",
    "                opeaks = batch[2].to(device)\n",
    "                rpeaks = batch[3].to(device)\n",
    "                fake_B_masked_opeaks = fake_B * (opeaks != 0)\n",
    "                fake_B_masked_rpeaks = fake_B * (rpeaks != 0)\n",
    "                opeak_count = torch.sum(opeaks != 0)\n",
    "                rpeak_count = torch.sum(rpeaks != 0)\n",
    "\n",
    "                loss_sample_opeaks = criterion_samplewise(\n",
    "                    fake_B_masked_opeaks, opeaks)\n",
    "                loss_sample_rpeaks = criterion_samplewise(\n",
    "                    fake_B_masked_rpeaks, rpeaks)\n",
    "                loss_sample = loss_sample_opeaks / opeak_count + \\\n",
    "                    rpeak_weight * loss_sample_rpeaks / rpeak_count\n",
    "            else:\n",
    "                loss_sample = criterion_samplewise(fake_B, real_B)\n",
    "\n",
    "            # Smooth the output with moving averages\n",
    "            if args.from_ppg:\n",
    "                fake_B = smoother(fake_B, device)\n",
    "\n",
    "            pred_fake = discriminator(fake_B, real_A)\n",
    "            loss_GAN = -torch.mean(pred_fake)\n",
    "\n",
    "            # Total loss\n",
    "            loss_G = loss_GAN + lambda_sample * loss_sample\n",
    "\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        else:\n",
    "            fake_B = generator(real_A)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        # optimizer_D.zero_grad()\n",
    "        for p in discriminator.parameters():\n",
    "            p.grad = None\n",
    "        # Real signals\n",
    "        real_validity = discriminator(real_B, real_A)\n",
    "        # Fake signals\n",
    "        fake_validity = discriminator(fake_B.detach(), real_A)\n",
    "        # Gradient penalty\n",
    "        gradient_penalty = compute_gradient_penalty(\n",
    "            discriminator, real_B, fake_B.detach(), real_A, patch, device)\n",
    "        \n",
    "        # Change Discriminator's loss to Least Squares Loss\n",
    "        real_loss = criterion_samplewise(discriminator(real_B), real_validity)\n",
    "        fake_loss = criterion_samplewise(discriminator(fake_B.detach()), fake_validity)\n",
    "        \n",
    "        # Adversarial loss\n",
    "        loss_D0 = 0.5*(torch.mean(real_loss) + torch.mean(fake_loss))\n",
    "        loss_D = loss_D0\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # --------------\n",
    "        #  Log Progress\n",
    "        # --------------\n",
    "\n",
    "        # Determine approximate time left\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        batches_left = args.n_epochs * len(dataloader) - batches_done\n",
    "        time_left = datetime.timedelta(\n",
    "            seconds=batches_left * (time.time() - prev_time))\n",
    "        prev_time = time.time()\n",
    "\n",
    "        # Print log\n",
    "        sys.stdout.write(\n",
    "            \"\\r[Epoch %d/%d] [Batch %d/%d] [D0 loss: %f] [D loss: %f] [G loss: %f, sample: %f, adv: %f] ETA: %s\"\n",
    "            % (epoch, args.n_epochs, i, len(dataloader),\n",
    "               loss_D0.item(), loss_D.item(), loss_G.item(), loss_sample.item(), loss_GAN.item(),\n",
    "               time_left)\n",
    "        )\n",
    "\n",
    "        writer.add_scalars('losses', {'g_loss': loss_G.item()}, batches_done)\n",
    "        writer.add_scalars('losses', {'d_loss': loss_D.item()}, batches_done)\n",
    "        writer.add_scalars(\n",
    "            'losses2', {'d_loss0': loss_D0.item()}, batches_done)\n",
    "        writer.add_scalars(\n",
    "            'losses2', {'gan_loss': loss_GAN.item()}, batches_done)\n",
    "        writer.add_scalars(\n",
    "            'losses3', {'sample_loss': loss_sample.item()}, batches_done)\n",
    "\n",
    "    if args.checkpoint_interval != -1 and epoch % args.checkpoint_interval == 0:\n",
    "        # Save model checkpoints\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'generator_state_dict': generator.state_dict(),\n",
    "            'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "        }, \"saved_models/%s/multi_models_%d.pth\" % (args.experiment_name, epoch))\n",
    "        sample_images(args.experiment_name, val_dataloader,\n",
    "                      generator, epoch, device)\n",
    "        evaluate_generated_signal_quality(\n",
    "            val_dataloader, generator, writer, epoch, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93b5a16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
